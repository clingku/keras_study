---
title: "Understanding recurrent neural networks"
output: 
  html_notebook: 
    theme: cerulean
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
***
RNN 레이어는 메모리를 가지고 있음 

RNN processes sequences by iterating through the sequence elements
and maintaining a state containing information relative to what it has seen so far.

***


![Figure 6.7 A recurrent network a network with a loop](images/6.7.png){width=300px}

***
#### Listing 6.20 More detailed pseudocode for the RNN

```{r eval=FALSE}
state_t <- 0
for (input_t in input_sequence) {
  output_t <- activation(dot(W, input_t) + dot(U, state_t) + b)
  state_t <- output_t
}
```

![Figure 6.8 A simple RNN, unrolled over time](images/6.8.png){width=600px}

***
### 6.2.1 A first recurrent layer in Keras

The process you just naively implemented in R corresponds to an actual Keras layer -- `layer_simple_rnn()`.

```{r eval=FALSE}
layer_simple_rnn(units = 32)
```

Simple RNN의 두 가지 모드

  * the full sequences of successive outputs for each timestep (3D (batch\_size, timesteps, output\_features))
  * only the last output for each input sequence (2D (batch\_size, output\_features))

```{r}
library(keras)
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32)

summary(model)
```

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE)

summary(model)
```

RNN층 여러개를 스택하면 네트워크의 표현력이 증대됨(물론 overfitting 주의)

```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32)  # This last layer only returns the last outputs.

summary(model)
```

***
#### Listing 6.22 Preparing the IMDB data

```{r}
library(keras)

max_features <- 10000  # Number of words to consider as features
maxlen <- 500  # Cuts off texts after this many words (among the max_features most common words)
batch_size <- 32

cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb 
cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences")

cat("Pad sequences (samples x time)\n")
input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
```

***
#### Listing 6.23 Training the model with embedding and simple RNN layers

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```


30s/epoch

Let's display the training and validation loss and accuracy:

```{r}
plot(history)
```

![Figure 6.9 Training and validation metrics on IMDB with layer_simple_rnn](images/6.9.png){width=500px}

Simple RNN를 사용한 결과는 84% 정확도

(3장에서의 정확도는 88% 이었음)

전체 시퀀스가 아닌 500 단어 제한으로 입력에 사용했기 때문

그리고 더 강력한 RNN 레이어 유형이 존재함

* LSTM
* GRU



***
### 6.2.2 Understanding the LSTM and GRU layers

LSTM (Long Short-Term Memory)

* 긴 시간에 걸친 의존성을 학습
* 오래된 시그널의 소실을 방지

![Figure 6.12 Anatomy of an LSTM](images/6.12.png){width=600px}

To a researcher, it seems that the choice of such constraints—the question of how to implement RNN cells—is better left to  ptimization algorithms (like genetic algorithms or reinforcement learning processes) than to human engineers. 

And in the future, that’s how we’ll build networks. In summary: you don’t need to understand anything about the specific  architecture of an LSTM cell; as a human, it shouldn’t be your job to understand it. 

__Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time, thus fighting the vanishing-gradient problem.__

***
### 6.2.3 A concrete LSTM example in Keras

케라스의 LSTM층은 출력 값(units value)만 지정하는 것을 추천

Keras has good defaults, and things will almost always “just work” without you having to spend time tuning parameters by hand.

#### Listing 6.27 Using the LSTM layer in Keras
```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

100s/epoch

```{r}
plot(history)
```

![Figure 6.13 Training and validation metrics on IMDB with LSTM](images/6.13.png){width=500px}

정확도는 3장에서와 비슷한 88%,

3장은 전체 텍스트를 이용했지만

여기서는 더 적은 자원(500 단어 시퀀스)를 이용함
